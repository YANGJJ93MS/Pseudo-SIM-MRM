{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_3_output_folder = 'algorithm_output'\n",
    "retrieved_filename = 'retrieved_result.csv'\n",
    "# extraction_keys_filename = 'environstd_inchi.csv'\n",
    "# extraction_keys_file_encoding = 'ANSI'\n",
    "\n",
    "foldername = 'GCMS_output' # this is the forlder where the cleaned data is stored\n",
    "cleaned_folder_name = 'RIgrouped_data'\n",
    "\n",
    "read_folder = os.path.join(os.getcwd(), foldername)\n",
    "cleaned_folder = os.path.join(read_folder, cleaned_folder_name) # folder that we will be reading data from\n",
    "# df = pd.read_csv(os.path.join(os.getcwd(), extraction_keys_filename), encoding=extraction_keys_file_encoding)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inchikey_list = df['InChIKey:'].str.strip().array\n",
    "# name_list = df['Name:'].str.strip().array\n",
    "columns_to_be_included = [\n",
    "    'Name:', \n",
    "    'Formula:',\n",
    "    'MW:',\n",
    "    'NIST#:',\n",
    "    'InChIKey:', \n",
    "    'MS2:',\n",
    "    'Score:',\n",
    "    'EstRI', #try to extract the 2 RI values in the text\n",
    "    'PredRI'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithm part from algorithm3\n",
    "##clean up the mw to valide python datatype\n",
    "#found some faulty data in the result datafile and modify the codes  August 4\n",
    "#work on 2021 August16 for modification for the RI grouped data\n",
    "def clean_MW(df):\n",
    "# def clean_precursormz(df):\n",
    "    inchikeys = df['InChIKey:'].unique().tolist()\n",
    "    for inchikey in inchikeys:\n",
    "        curr_precursormz = df.loc[df['InChIKey:'] == inchikey, 'MW:'].iloc[0]\n",
    "        try:\n",
    "            curr_precursormz = ast.literal_eval(curr_precursormz)\n",
    "            df.loc[df['InChIKey:'] == inchikey, ['MW:']] = curr_precursormz[0]\n",
    "        except TypeError:\n",
    "            curr_precursormz = float(curr_precursormz)\n",
    "            df.loc[df['InChIKey:'] == inchikey, ['MW:']] = curr_precursormz\n",
    "        except ValueError:\n",
    "            curr_precursormz = float(curr_precursormz)\n",
    "            df.loc[df['InChIKey:'] == inchikey, ['MW:']] = curr_precursormz\n",
    "    return df\n",
    "\n",
    "def retrieve_ms2_dict(df): \n",
    "    ms2_dict = pd.Series(df['Score:'].array, index=df['MS2:']).to_dict()\n",
    "    return ms2_dict\n",
    "\n",
    "def filter_score(df, threshold):\n",
    "    return df[df['Score:'] >= threshold]\n",
    "\n",
    "def filter_ms2(df, msthreshold):\n",
    "    return df[df['MS2:'] >= msthreshold]\n",
    "\n",
    "def append_to_list(df):\n",
    "    output_dict = {}\n",
    "    for i, item in df['Name:'].items():\n",
    "        if 'Name:' in output_dict.keys():\n",
    "            if item not in output_dict['Name:']:\n",
    "                if type(output_dict['Name:']) == list:\n",
    "                    if type(item) == list:\n",
    "                        output_dict['Name:'] += item\n",
    "                    else:\n",
    "                        output_dict['Name:'].append(item)\n",
    "                else:\n",
    "                    if type(item) == list:\n",
    "                        output_dict['Name:'] = item.append(output_dict['Name:'])\n",
    "                    else:\n",
    "                        output_dict['Name:'] = [output_dict['Name:'], item]\n",
    "        else:\n",
    "            output_dict['Name:'] = item\n",
    "    return pd.Series(output_dict)\n",
    "#saving compound name in the 'Name:' column, some list of names inside the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# markdown: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#set search range for the similar product ms\n",
    "search_range = 1\n",
    "threshold =100 #threshold for ms2 score\n",
    "msthreshold = 10 #threshold for ms2 \n",
    "\n",
    "output_folder = os.path.join(os.getcwd(), algo_3_output_folder, 'non_combined_output')\n",
    "\n",
    "if not (os.path.exists(output_folder)):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "## get list of csv file names\n",
    "cwd = os.getcwd()\n",
    "os.chdir(cleaned_folder)\n",
    "listdir = glob.glob('*.csv')\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from below, precursormz === mw  \n",
    "# start = time.time()\n",
    "for filename in tqdm(listdir):\n",
    "    precursormz = int(filename.split('_')[0])  \n",
    "    cleaned_file = os.path.join(cleaned_folder, filename)\n",
    "    cleaned_df = pd.read_csv(cleaned_file)\n",
    "    #sort the datafile to pop the most abundance ms2 to the top \n",
    "    cleaned_df.sort_values(by=['InChIKey:', 'Score:'], inplace=True)     \n",
    "    #clean up the precursor ion or the mw, make it into python datatype\n",
    "    cleaned_df = clean_MW(cleaned_df)\n",
    "    cleaned_df['Score:'] = pd.to_numeric(cleaned_df['Score:'])\n",
    "    cleaned_df['MS2:'] = pd.to_numeric(cleaned_df['MS2:'])\n",
    "    cleaned_df = filter_score(cleaned_df, threshold)  ##limits of potential product ions for calculation in a short time\n",
    "    cleaned_df = filter_ms2(cleaned_df, msthreshold)  # cleaned_df = filter_ms2(cleaned_df, msthreshold)  #\n",
    "    \n",
    "#calculate the precision and recall inside one file\n",
    "    output_file = os.path.join(output_folder, str(precursormz) + '_algo3.csv')\n",
    "    inchikeys = cleaned_df['InChIKey:'].unique().tolist()\n",
    "#if no interfering items mean the compound is only one detected which is faulty data, break out the loop\n",
    "    if len(inchikeys) <=1:\n",
    "        continue\n",
    "    \n",
    "    TP, FN = 1,0\n",
    "    header_flag = True\n",
    "    \n",
    "    end1 = time.time()\n",
    "#     print(\"First elapsed:\", end1-start)\n",
    "\n",
    "    for inchikey in inchikeys:\n",
    "#put all ms2 lists of one compound all in the target_item_df by inchikey separation \n",
    "        target_item_df = cleaned_df[cleaned_df['InChIKey:'] == inchikey].copy()  \n",
    "        interfering_items_df = cleaned_df[cleaned_df['InChIKey:'] != inchikey]\n",
    "            \n",
    "        interfering_items_s = interfering_items_df.groupby(['InChIKey:']).apply(retrieve_ms2_dict)\n",
    "        target_ms2_array = target_item_df['MS2:'].array\n",
    "        output_dict = {}\n",
    "        end2 = time.time()\n",
    "#         print(\"Second elapsed:\", end2-end1)\n",
    "        \n",
    "#start the tn fp calculation#\n",
    "# ====================================================================================================================== #\n",
    "        for ms2 in target_ms2_array:\n",
    "                FP, TN = 0, 0\n",
    "                for i, ms2_dict in interfering_items_s.iteritems():\n",
    "                    TN_flag = True\n",
    "                    \n",
    "                    if (int(ms2) - int(max(ms2_dict.keys()))) > 1:\n",
    "                        TN += 1\n",
    "                        continue\n",
    "\n",
    "#make keys into array and using broadcasting by minus to count the number of [-1,0,1]\n",
    "#make ms2 array\n",
    "#                     a = np.array([ms2])\n",
    "#                     b = np.array([*ms2_dict])\n",
    "#                     result = np.subtract(b,a)\n",
    "#get rid of the for loop with if conditional sentences\n",
    "#count the number of [-1,0,1]\n",
    "#                     count = 0\n",
    "#                     for i in range(len(result)):\n",
    "#                         if result[i] == 0:\n",
    "#                             count +=1\n",
    "#                         elif result [i] == 1:\n",
    "#                             count +=1\n",
    "#                         elif result [i] == -1:\n",
    "#                             count +=1\n",
    "#                     unique, counts = np.unique(result, return_counts = True)\n",
    "#                     resultdict = dict(zip(unique,counts))\n",
    "#                     count=0\n",
    "#                     if 1 in resultdict.keys():\n",
    "#                         count+=1\n",
    "#                     elif 0 in resultdict.keys():\n",
    "#                         count +=1\n",
    "#                     elif -1 in resultdict.keys():\n",
    "#                         count+=1\n",
    "#                     else:\n",
    "#                         continue\n",
    "                        \n",
    "#                     if count >=1:\n",
    "#                         FP+=1\n",
    "#                     else:\n",
    "#                         TN+=1\n",
    "#                     FP = resultdict['-1'] + resultdict['0'] + resultdict['1']\n",
    "#                     TN = N - FP\n",
    "                    # N is the number of ms2 list in the interfering target dataframes  \n",
    "                    \n",
    "#                     end3 = time.time()\n",
    "#                     print(\"Third elapsed:\", end3-end2)\n",
    "                    for key in ms2_dict.keys():\n",
    "                        diff = round(int(ms2) - int(key), 2)\n",
    "                        if abs(diff) <= search_range:\n",
    "                            TN_flag = False\n",
    "                            FP += 1\n",
    "                        elif diff < 0:\n",
    "                            if (TN_flag):\n",
    "                                TN += 1\n",
    "                            break\n",
    "                \n",
    "#                 end4 = time.time()\n",
    "#                 print(\"4th elapsed:\", end4-end3)\n",
    "                output_dict[ms2] = {\n",
    "                'TP': TP,\n",
    "                'FN': FN,\n",
    "                'TN': TN,\n",
    "                'FP': FP,\n",
    "                'Accuracy': (TP+TN) / (TP+TN+FP+FN),\n",
    "                'Sensitivity': TP / (TP+FN),\n",
    "                'Specificity': TN / (FP+TN)\n",
    "                }\n",
    "        output_df = pd.DataFrame.from_dict(output_dict, orient='index')\n",
    "        output_df = target_item_df.join(output_df, on='MS2:')\n",
    "\n",
    "        output_df.sort_values(by=['Specificity'], inplace=True)\n",
    "        output_df.to_csv(output_file, index=False, mode='a', header=header_flag)\n",
    "        header_flag = False #only include header for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the algoirthm for the whole dataset need 67hrs to finish the calculation 2022-3-18 17:42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all output into same csv file\n",
    "read_folder = os.path.join(os.getcwd(), algo_3_output_folder, 'non_combined_output')\n",
    "output_folder = os.path.join(os.getcwd(), algo_3_output_folder)\n",
    "cwd = os.getcwd()\n",
    "os.chdir(read_folder)\n",
    "listdir = glob.glob('*.csv')\n",
    "os.chdir(cwd)\n",
    "\n",
    "output_filename = os.path.join(output_folder, 'combined_output_algo_3.csv')\n",
    "\n",
    "combined_csv = pd.concat([ pd.read_csv(os.path.join(read_folder, f)) for f in tqdm(listdir) ], ignore_index=True, sort=False)\n",
    "combined_csv.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove the repeated inchikey and reoutput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retreive compounds from the combined result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo_3_output_folder = 'algorithm_3_output'\n",
    "retrieved_filename = 'retrieved_result.csv'\n",
    "extraction_keys_filename = 'availchem_inchi.csv'\n",
    "extraction_keys_file_encoding = 'ANSI'\n",
    "algo_3_result_filename = 'combined_output_algo_3.csv'\n",
    "\n",
    "df = pd.read_csv(os.path.join(os.getcwd(), extraction_keys_filename), encoding=extraction_keys_file_encoding)\n",
    "df.columns = ['name','cas','smiles','mw','InChiKey']\n",
    "inchikey_list = df['InChiKey'].str.strip().array\n",
    "name_list = df['name'].str.strip().array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve result from inchikey list\n",
    "\n",
    "algo_3_output_folder = os.path.join(os.getcwd(), algo_3_output_folder)\n",
    "retrieved_output_filename = os.path.join(algo_3_output_folder, retrieved_filename)\n",
    "\n",
    "df = pd.read_csv(os.path.join(algo_3_output_folder, algo_3_result_filename))\n",
    "    \n",
    "output_df = df[df['InChIKey:'].isin(inchikey_list)]\n",
    "# for name in name_list:\n",
    "#     try:\n",
    "#         #append isomers by adding different name\n",
    "#         output_df = output_df.append(df[df['Name:'].str.contains(name, case=True, regex=False)], ignore_index=True)\n",
    "#     except KeyError:\n",
    "#         pass\n",
    "    \n",
    "output_df.drop(labels=['TP', 'FP', 'TN', 'FN'], inplace=True, axis=1)\n",
    "output_df.to_csv(retrieved_output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
